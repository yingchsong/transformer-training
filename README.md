# Transformer æ¨¡å‹å®ç°è¯´æ˜æ–‡æ¡£

æœ¬é¡¹ç›®åŸºäº PyTorch å®ç°äº†ç»å…¸çš„ **Transformer** æ¶æ„ï¼ˆ[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)ï¼‰ï¼ŒåŒ…å«å®Œæ•´çš„ç¼–ç å™¨ï¼ˆEncoderï¼‰ã€è§£ç å™¨ï¼ˆDecoderï¼‰ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç ã€åµŒå…¥å±‚å’Œç”Ÿæˆå™¨æ¨¡å—ã€‚ä»¥ä¸‹å°†è¯¦ç»†è¯´æ˜ä»è¾“å…¥åˆ°è¾“å‡ºçš„æ•´ä¸ªæµç¨‹ï¼Œä»¥åŠå„ç»„ä»¶çš„è¾“å…¥/è¾“å‡ºç»“æ„ã€‚

---

## ğŸ“Œ æ•´ä½“æ¶æ„æ¦‚è§ˆ

```text
[Source Tokens] â”€â”€(src_embed + pos_enc)â”€â”€â–º Encoder â”€â”€â–º Memory (Context)
                                                           â”‚
[Tgt Tokens]   â”€â”€(tgt_embed + pos_enc)â”€â”€â–º Decoder â—„â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                    Generator (Linear + log_softmax)
                                                           â†“
                                                [Log-probabilities over vocab]
```

æ¨¡å‹ç”± `EncoderDecoder` ç±»ç»Ÿä¸€ç®¡ç†ï¼Œå…¶å‰å‘ä¼ æ’­æ¥å£ä¸ºï¼š

```python
output = model(src, tgt, src_mask, tgt_mask)
```

---

## ğŸ”¤ è¾“å…¥è¯´æ˜

| è¾“å…¥é¡¹       | å¼ é‡å½¢çŠ¶                     | å«ä¹‰ |
|--------------|------------------------------|------|
| `src`        | `[batch_size, src_seq_len]`  | æºåºåˆ—ï¼ˆå¦‚è‹±æ–‡å¥å­ï¼‰çš„ token ID |
| `tgt`        | `[batch_size, tgt_seq_len]`  | ç›®æ ‡åºåˆ—ï¼ˆå¦‚ä¸­æ–‡ç¿»è¯‘ï¼‰çš„ token IDï¼ˆè®­ç»ƒæ—¶ä¸ºå³ç§»ä¸€ä½çš„ ground truthï¼‰ |
| `src_mask`   | `[batch_size, 1, src_seq_len]` | æºåºåˆ— padding æ©ç ï¼ˆ1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œ0 è¡¨ç¤º paddingï¼‰ |
| `tgt_mask`   | `[batch_size, tgt_seq_len, tgt_seq_len]` | ç›®æ ‡åºåˆ—æ©ç ï¼šç»“åˆ padding æ©ç  + **æœªæ¥ä½ç½®æ©ç **ï¼ˆé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼‰ |

> ğŸ’¡ `tgt_mask` é€šå¸¸ç”± `subsequent_mask(tgt_seq_len)` ä¸ padding æ©ç æŒ‰ä½â€œä¸â€å¾—åˆ°ã€‚

---

## ğŸ§  ç¼–ç å™¨ï¼ˆEncoderï¼‰

### ç»“æ„ç»„æˆ
- ç”± `N` ä¸ªç›¸åŒçš„ `EncoderLayer` å †å è€Œæˆã€‚
- æ¯ä¸ª `EncoderLayer` åŒ…å«ï¼š
  1. **å¤šå¤´è‡ªæ³¨æ„åŠ›å­å±‚**ï¼ˆMulti-Head Self-Attentionï¼‰
  2. **å‰é¦ˆç½‘ç»œå­å±‚**ï¼ˆPosition-wise Feed-Forward Networkï¼‰
- æ¯ä¸ªå­å±‚åæ¥ **æ®‹å·®è¿æ¥ + LayerNorm**ï¼ˆé€šè¿‡ `SublayerConnection` å®ç°ï¼‰ã€‚
- æœ€ç»ˆè¾“å‡ºå‰å†è¿›è¡Œä¸€æ¬¡ `LayerNorm`ã€‚

### è¾“å…¥
- **è¯åµŒå…¥ + ä½ç½®ç¼–ç **ï¼š  
  `x = src_embed(src) + positional_encoding`  
  å½¢çŠ¶ï¼š`[batch_size, src_seq_len, d_model]`
- **æºæ©ç  `src_mask`**ï¼šç”¨äºå±è”½ padding ä½ç½®ã€‚

### è¾“å‡º
- **Memoryï¼ˆè¯­ä¹‰è®°å¿†ï¼‰**ï¼š  
  å½¢çŠ¶ï¼š`[batch_size, src_seq_len, d_model]`  
  åŒ…å«æ•´ä¸ªæºåºåˆ—çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨å¾ï¼Œä¾› Decoder çš„ cross-attention ä½¿ç”¨ã€‚

---

## ğŸ” è§£ç å™¨ï¼ˆDecoderï¼‰

### ç»“æ„ç»„æˆ
- ç”± `N` ä¸ªç›¸åŒçš„ `DecoderLayer` å †å è€Œæˆã€‚
- æ¯ä¸ª `DecoderLayer` åŒ…å«ä¸‰ä¸ªå­å±‚ï¼š
  1. **æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼ˆMasked Multi-Head Self-Attentionï¼‰  
     â†’ ä»…å…è®¸å…³æ³¨å½“å‰åŠä¹‹å‰çš„ä½ç½®ï¼ˆé˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰
  2. **ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›**ï¼ˆMulti-Head Cross-Attentionï¼‰  
     â†’ Query æ¥è‡ª Decoderï¼ŒKey/Value æ¥è‡ª Encoder çš„ memory
  3. **å‰é¦ˆç½‘ç»œ**ï¼ˆPosition-wise Feed-Forward Networkï¼‰
- æ¯ä¸ªå­å±‚ååŒæ ·æ¥ **æ®‹å·®è¿æ¥ + LayerNorm**ã€‚
- æœ€ç»ˆè¾“å‡ºå‰è¿›è¡Œä¸€æ¬¡ `LayerNorm`ã€‚

### è¾“å…¥
- **ç›®æ ‡åµŒå…¥ + ä½ç½®ç¼–ç **ï¼š  
  `x = tgt_embed(tgt) + positional_encoding`  
  å½¢çŠ¶ï¼š`[batch_size, tgt_seq_len, d_model]`
- **Memory**ï¼šæ¥è‡ª Encoder çš„è¾“å‡º
- **`src_mask`**ï¼šç”¨äº cross-attention ä¸­å±è”½æº padding
- **`tgt_mask`**ï¼šç”¨äº self-attention ä¸­å±è”½æœªæ¥ä½ç½®å’Œç›®æ ‡ padding

### è¾“å‡º
- **è§£ç å™¨ç‰¹å¾è¡¨ç¤º**ï¼š  
  å½¢çŠ¶ï¼š`[batch_size, tgt_seq_len, d_model]`  
  æ¯ä¸ªä½ç½®çš„å‘é‡èåˆäº†ï¼š
  - ç›®æ ‡åºåˆ—çš„å†å²ä¿¡æ¯ï¼ˆé€šè¿‡ masked self-attnï¼‰
  - æºåºåˆ—çš„ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆé€šè¿‡ cross-attnï¼‰

---

## ğŸ¯ ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰

- ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚ + `log_softmax`ï¼š
  ```python
  log_probs = log_softmax(linear(x), dim=-1)
  ```
- **è¾“å…¥**ï¼šDecoder è¾“å‡ºï¼Œ`[batch_size, tgt_seq_len, d_model]`
- **è¾“å‡º**ï¼šå¯¹æ•°æ¦‚ç‡åˆ†å¸ƒï¼Œ`[batch_size, tgt_seq_len, vocab_size]`

> âš ï¸ æ³¨æ„ï¼šè®­ç»ƒæ—¶é€šå¸¸ä½¿ç”¨ `CrossEntropyLoss` æˆ– `NLLLoss`ï¼Œå› æ­¤è¾“å‡ºä¸º **log-probabilities** è€Œé raw logitsã€‚

---

## ğŸ“¦ åµŒå…¥ä¸ä½ç½®ç¼–ç 

### `Embeddings`
- å°† token ID æ˜ å°„ä¸º `d_model` ç»´å‘é‡ã€‚
- **å…³é”®ç»†èŠ‚**ï¼šåµŒå…¥å‘é‡ä¹˜ä»¥ `âˆšd_model`ï¼ˆè®ºæ–‡å»ºè®®ï¼Œä¿æŒä¸ä½ç½®ç¼–ç ç›¸åŒé‡çº§ï¼‰ã€‚

### `PositionalEncoding`
- ä½¿ç”¨ **å›ºå®šæ­£å¼¦/ä½™å¼¦å‡½æ•°** ç¼–ç ä½ç½®ä¿¡æ¯ï¼ˆæ— éœ€å­¦ä¹ ï¼‰ã€‚
- æ”¯æŒæœ€é•¿ `max_len=5000` çš„åºåˆ—ã€‚
- ä¸è¯åµŒå…¥ç›¸åŠ åç» Dropout è¾“å‡ºã€‚

---

## ğŸ§© æ³¨æ„åŠ›æœºåˆ¶

### `MultiHeadedAttention`
- æ”¯æŒè‡ªæ³¨æ„åŠ›ï¼ˆself-attnï¼‰å’Œäº¤å‰æ³¨æ„åŠ›ï¼ˆcross-attnï¼‰ã€‚
- è¾“å…¥ Q/K/V å¯æ¥è‡ªä¸åŒæ¥æºï¼ˆå¦‚ Decoder ä¸­ Q æ¥è‡ªè‡ªèº«ï¼ŒK/V æ¥è‡ª memoryï¼‰ã€‚
- å†…éƒ¨è°ƒç”¨ `attention()` å‡½æ•°å®ç° **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**ï¼ˆScaled Dot-Product Attentionï¼‰ã€‚
- æ”¯æŒä»»æ„æ©ç ï¼ˆpadding / future maskingï¼‰ã€‚

---

## âœ… ä½¿ç”¨ç¤ºä¾‹ï¼ˆä¼ªä»£ç ï¼‰

```python
# åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶
encoder = Encoder(EncoderLayer(...), N=6)
decoder = Decoder(DecoderLayer(...), N=6)
src_embed = nn.Sequential(Embeddings(d_model, src_vocab), PositionalEncoding(...))
tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), PositionalEncoding(...))
generator = Generator(d_model, tgt_vocab)

model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)

# å‰å‘ä¼ æ’­
src = torch.randint(0, src_vocab, (32, 10))      # batch=32, src_len=10
tgt = torch.randint(0, tgt_vocab, (32, 8))       # tgt_len=8
src_mask = (src != PAD).unsqueeze(1)             # [32, 1, 10]
tgt_mask = make_std_mask(tgt)                    # [32, 8, 8] (å« future mask)

output = model(src, tgt, src_mask, tgt_mask)     # [32, 8, d_model]
log_probs = model.generator(output)              # [32, 8, tgt_vocab]
```

---

## ğŸ“š æ€»ç»“

| æ¨¡å— | è¾“å…¥ | è¾“å‡º | å…³é”®ä½œç”¨ |
|------|------|------|--------|
| **Encoder** | `[B, S, d]`, `src_mask` | `[B, S, d]` | æå–æºåºåˆ—å…¨å±€è¯­ä¹‰ |
| **Decoder** | `[B, T, d]`, `memory`, `src_mask`, `tgt_mask` | `[B, T, d]` | è‡ªå›å½’ç”Ÿæˆç›®æ ‡åºåˆ— |
| **Generator** | `[B, T, d]` | `[B, T, V]` | æ˜ å°„åˆ°è¯è¡¨æ¦‚ç‡ |

> å…¶ä¸­ï¼š`B = batch_size`, `S = src_seq_len`, `T = tgt_seq_len`, `d = d_model`, `V = vocab_size`

æœ¬å®ç°ä¸¥æ ¼éµå¾ªåŸå§‹è®ºæ–‡ç»“æ„ï¼Œå¯ä½œä¸ºæ•™å­¦ã€ç ”ç©¶æˆ–è‡ªå®šä¹‰æ‰©å±•çš„åŸºç¡€ã€‚

## ğŸ“– å‚è€ƒèµ„æ–™

Rush, A. M. (2018). The annotated transformer. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS) (pp. 52â€“60). Association for Computational Linguistics. https://aclanthology.org/W18-2509/